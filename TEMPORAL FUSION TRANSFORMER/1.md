```
import os
import copy
import torch
import warnings
import numpy as np
import pandas as pd
from pathlib import Path
import pytorch_lightning as pl
from pytorch_forecasting.data import GroupNormalizer
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_forecasting.data.examples import get_stallion_data
from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss
from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor
from pytorch_forecasting import Baseline, TemporalFusionTransformer, 
TimeSeriesDataSet
from pytorch_forecasting.models.temporal_fusion_transformer.tuning import 
optimize_hyperparameters
warnings.filterwarnings("ignore")
```

## EXPLANATION OF CODES AND THEIR THEORÄ°CAL EXPLANATION

### pytorch_lightning: 

**PyTorch Lightning** is a lightweight, PyTorch-based framework designed to make your deep learning projects more efficient and organized. Here are its main advantages:

Less Boilerplate Code: It eliminates repetitive, standard code for tasks like model training loops, optimization setups, and distributed training. This allows developers to focus on the model's architecture and experiment logic.

Modular & Organized Structure: Using classes like LightningModule, it enables you to define your model, optimization strategy, and training/validation/testing steps in one place, improving code readability and organization.

Easy Scalability: You can effortlessly scale your code from a single GPU to multiple GPUs, TPUs, or distributed systems without significant code changes, ensuring hardware independence.

Reproducibility: By managing random seeds and standardizing experiment outputs, it helps ensure the reproducibility of deep learning experiments.

Automated Training Routines: The Trainer class automatically manages the training, validation, testing, and prediction processes, reducing the need for manual loop writing and minimizing errors.

### pytorch_lightning.loggers

The **pytorch_lightning.loggers module** contains various logging tools for PyTorch Lightning. When training deep learning models, it's crucial to track the model's performance (metrics like loss and accuracy), hyperparameters, distribution of model weights, training progress, and even visualizations (e.g., generated images). The loggers module provides a flexible interface for saving this data in different formats and across various platforms.

The TensorBoardLogger, for instance, creates log files compatible with TensorBoard, developed by Google.

A log file is a file where a computer system, application, or network device records specific events, processes, or messages in chronological order. They are typically in plain text format, with each line representing an event that occurred at a specific time.

Log files are essential for:

Debugging: Identifying the root cause of issues when software crashes or doesn't behave as expected.

Performance Analysis: Gaining insights into system or application performance, helping to spot bottlenecks and optimize.

Security Auditing: Recording security events like unauthorized access attempts or suspicious activities for detection and analysis of potential cyberattacks.

Process Tracking: Monitoring specific operations performed by users or systems.

Audit and Compliance: Meeting legal requirements for logging and storing certain operations in regulated industries.

### pytorch_lightning.callbacks
Callbacks are classes in PyTorch Lightning that allow specific code to be automatically executed at different stages of the Trainer class (e.g., before training starts, at each step, at the end of each epoch). They help make your project more modular and reusable by separating the core training logic (model definition, optimization) from your LightningModule.

EarlyStopping is a widely used technique when training deep learning models. It primarily helps with:

Overfitting Prevention: It automatically stops training if the model's validation performance doesn't improve for a certain period, preventing overfitting, saving resources, and ensuring the model stops at its best generalization performance.

Resource Optimization: It prevents unnecessary GPU/CPU time and energy waste when there's no significant improvement in training performance.

LearningRateMonitor automatically tracks and logs the learning rates used during training. Its benefits include:

Learning Rate Tracking: Helps you see how learning rates change throughout training, especially when using learning rate schedulers.

Debugging and Optimization: Allows you to verify if learning rates are behaving as expected, aiding in identifying and resolving optimization issues.

Visualization: Logged learning rates can be easily visualized in tools like TensorBoard, providing insights into your learning rate schedule's effectiveness.

### pytorch_forecasting
The pytorch_forecasting library is a high-level PyTorch library specifically designed for time series forecasting.

TimeSeriesDataSet is the core data structuring class within pytorch_forecasting. It's responsible for transforming complex time series data (which often includes multiple time series, missing values, varying lengths, categorical and numerical variables, and known/unknown future covariates) into a suitable format for machine learning models. TimeSeriesDataSet abstracts away this complexity, preparing the data for easy model consumption.

The Baseline class provides a simple reference model for time series forecasting. It's typically used as a benchmark to evaluate the performance of more complex deep learning models. If a model can't outperform a basic baseline, it suggests either poor configuration or that the model isn't suitable for the problem at hand.

The TemporalFusionTransformer (TFT) is a powerful and interpretable deep learning model specifically designed for time series forecasting. Developed by Google, it's known for its ability to effectively handle various covariate types (static, time-varying known/unknown) and its use of attention mechanisms for predictions.

```
data = get_stallion_data()
# add time index
data["time_idx"] = data["date"].dt.year * 12 + data["date"].dt.month   # data["date"] = ["2021-01-01", "2021-02-01", "2022-01-01"] ===>  [2021*12 + 1, 2021*12 + 2, 2022*12 + 1] = [24253, 24254, 24265]
data["time_idx"] -= data["time_idx"].min()   # is a normalization step applied to the time index. Let me break it down clearly: data["time_idx"] = [24253, 24254, 24265]; min_time_idx = 24253, data["time_idx"] -= 24253; data["time_idx"] = [0, 1, 12]
# add additional features
data["month"] = data.date.dt.month.astype(str).astype("category") # To extract the month from the date column and convert it into a categorical variable, str: Converts the month integer (e.g. 4) to a string ('4'). astype(category): Converts the string into a pandas categorical type.
data["log_volume"] = np.log(data.volume + 1e-8)
data["avg_volume_by_sku"] = data.groupby(["time_idx", "sku"], 
observed=True).volume.transform("mean")
data["avg_volume_by_agency"] = data.groupby(["time_idx", "agency"], 
observed=True).volume.transform("mean")
# we want to encode special days as one variable and thus need to first reverse one-hot encoding
special_days = [
 "easter_day",
 "good_friday",
 "new_year",
 "christmas",
 "labor_day",
 "independence_day",
 "revolution_day_memorial",
 "regional_games",
 "fifa_u_17_world_cup",
 "football_gold_cup",
 "beer_capital",
 "music_fest",
]
data[special_days] = data[special_days].apply(lambda x: x.map({0: "-", 1: x.name})).astype("category")
data.sample(10, random_state=521) ```
